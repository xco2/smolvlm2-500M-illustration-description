from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel, LoraConfig
import torch
import os
import json
import pandas as pd
from transformers import TextStreamer
from PIL import Image
import numpy as np
import inspect
import yaml


class AttentionViewer:
    def __init__(self, config_path="attention_view_config.yaml"):
        """
        åˆå§‹åŒ– AttentionViewer ç±»
        
        Args:
            config_path: YAMLé…ç½®æ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config = self._load_config(config_path)
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ["HF_ENDPOINT"] = self.config["environment"]["hf_endpoint"]

        # è®¾ç½®æ¨¡å‹è·¯å¾„
        self.base_model_path = self.config["model"]["base_model_path"]
        self.lora_step = self.config["model"]["lora_step"]
        self.SMOL = self.config["model"]["SMOL"]

        # å¦‚æœæœªæä¾›lora_adapter_pathï¼Œåˆ™è‡ªåŠ¨ç”Ÿæˆ
        lora_adapter_path = self.config["model"]["lora_adapter_path"]
        if lora_adapter_path is None and self.lora_step > 0:
            if self.SMOL:
                template = self.config["model"]["small_lora_path_template"]
                self.lora_adapter_path = template.format(lora_step=self.lora_step)
            else:
                template = self.config["model"]["large_lora_path_template"]
                self.lora_adapter_path = template.format(lora_step=self.lora_step)
                # æš‚ä¸æ”¯æŒ2Bæ¨¡å‹çš„attentionå¯è§†åŒ–
                raise NotImplementedError("2Bæ¨¡å‹çš„attentionå¯è§†åŒ–æš‚ä¸æ”¯æŒ")
        else:
            self.lora_adapter_path = lora_adapter_path

        self.save_path = self.config["output"]["save_path"]
        os.makedirs(self.save_path, exist_ok=True)

        # åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹
        self.processor = AutoProcessor.from_pretrained(self.base_model_path)
        self.load_model()

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        self.print_model_info()
    
    def _load_config(self, config_path):
        """åŠ è½½YAMLé…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config

    def load_model(self):
        """åŠ è½½æ¨¡å‹ï¼Œæ ¹æ®lora_stepå†³å®šæ˜¯å¦ä½¿ç”¨LoRA"""
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model_for_lora = AutoModelForImageTextToText.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.bfloat16,
            # _attn_implementation="flash_attention_2"
        )

        if self.lora_step > 0:
            print("åŠ è½½LORA...")
            # åŠ è½½LoRAé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹
            model = PeftModel.from_pretrained(base_model_for_lora, self.lora_adapter_path)

            # å°†æœ€ç»ˆçš„æ¨¡å‹ç§»åŠ¨åˆ°CUDAè®¾å¤‡
            model = model.to("cuda").to(torch.bfloat16)

            # åˆå¹¶LoRAæƒé‡åˆ°åŸºç¡€æ¨¡å‹
            model = model.merge_and_unload().eval()
        else:
            print("ä½¿ç”¨åŸºç¡€æ¨¡å‹...")
            model = base_model_for_lora
            model = model.to("cuda").to(torch.bfloat16).eval()

        self.model = model

    def print_model_info(self):
        """æ‰“å°æ¨¡å‹ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ˜¾å­˜å ç”¨ã€å‚æ•°é‡ç­‰"""
        peak_mem = torch.cuda.max_memory_allocated()
        print(f"æ¨¡å‹å½“å‰å ç”¨æ˜¾å­˜: {peak_mem / 1024 ** 3:.2f} GB")
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"æ€»å‚æ•°: {total_params / 1e5:.2f} ä¸‡")
        print("-" * 60)

        print("model")
        print(type(self.model))
        file_path = inspect.getfile(self.model.__class__)
        print(file_path)
        print("-" * 60)
        print("text model")
        print(type(self.model.model.text_model))
        file_path = inspect.getfile(self.model.model.text_model.__class__)
        print(file_path)
        print("-" * 60)

    def generate_with_attention(self, image_path, prompt_text=None):
        """
        ç”Ÿæˆå¸¦æœ‰æ³¨æ„åŠ›ä¿¡æ¯çš„æ–‡æœ¬
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            prompt_text: æç¤ºæ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æç¤º
            
        Returns:
            generated_texts: ç”Ÿæˆçš„æ–‡æœ¬
            attentions: æ³¨æ„åŠ›ä¿¡æ¯
        """
        if prompt_text is None:
            prompt_text = self.config["generation"]["default_prompt"]

        with torch.no_grad():
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "url": image_path},
                        {"type": "text", "text": prompt_text},
                    ]
                },
            ]

            inputs = self.processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            ).to(self.model.device, dtype=torch.bfloat16)

            print(f'[Image]: {image_path}')

            # ä»é…ç½®ä¸­è·å–ç”Ÿæˆå‚æ•°
            gen_config = self.config["generation"]
            generated_ids = self.model.generate(
                **inputs,
                num_beams=gen_config["num_beams"],
                max_length=gen_config["max_length"],
                early_stopping=gen_config["early_stopping"],
                no_repeat_ngram_size=gen_config["no_repeat_ngram_size"],
                length_penalty=gen_config["length_penalty"],
                output_attentions=True,
                return_dict_in_generate=True
            )

            attentions = generated_ids.attentions
            self._print_attention_info(attentions)

            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated_texts = self.processor.batch_decode(
                generated_ids.sequences,
                skip_special_tokens=True,
            )
            print(f'ğŸ¤–ï¸: {generated_texts}')
            print("-" * 50)

            return generated_texts, generated_ids

    def _print_attention_info(self, attentions):
        """æ‰“å°æ³¨æ„åŠ›ä¿¡æ¯"""
        att_len_str = f"{len(attentions)} å­—"
        print(att_len_str)
        if len(attentions) > 0:
            layer_str = f"{len(attentions[0])} å±‚"
            print(layer_str)
            if len(attentions[0]) > 0:
                att_map_str = f"att map: {attentions[0][0].shape}"
                print(att_map_str)

    def process_attention_maps(self, generated_ids):
        """
        å¤„ç†æ³¨æ„åŠ›å›¾
        
        Args:
            generated_ids: ç”Ÿæˆçš„token IDså’Œæ³¨æ„åŠ›ä¿¡æ¯
            
        Returns:
            block_index: å›¾åƒå—ç´¢å¼•ä¿¡æ¯
            max_row: æœ€å¤§è¡Œæ•°
            max_col: æœ€å¤§åˆ—æ•°
            attentions: æ³¨æ„åŠ›ä¿¡æ¯
        """
        attentions = generated_ids.attentions
        all_tokens = generated_ids.sequences[0].cpu().numpy()

        # æ‰¾åˆ‡å‰²ä¸ºæ¨ªç«–å¤šå°‘ä¸ªæ ¼å­
        # resizeæˆ512
        # ä¸€å¼ å›¾å˜8*8ä¸ªtoken
        # æ‰€ä»¥ä¸€ä¸ªtokenå¯¹åº”512/8=64
        # æ‰€ä»¥ä¸€ä¸ªæ ¼å­å¯¹åº”64*64ä¸ªåƒç´ 
        special_tokens = [f"<row_{i}_col_{j}>" for i in range(1, 5) for j in range(1, 5)]
        special_tokens += ["<fake_token_around_image>", "<global-img>"]

        image_special_tokens = {k: self.processor.tokenizer.added_tokens_encoder[k] for k in special_tokens}
        fake_tokens_index = np.where(all_tokens == image_special_tokens["<fake_token_around_image>"])[0]

        block_index = []
        # è®¡ç®—æ¯ä¸ªblockçš„ä½ç½®
        max_row = 0
        max_col = 0
        prompt_start_index = fake_tokens_index[-1] + 1

        # å¤„ç†æ¯ä¸ªå›¾åƒå—
        for f in fake_tokens_index[:-1]:
            position_token = all_tokens[f + 1]
            row = None
            col = None
            for k in image_special_tokens.keys():
                if image_special_tokens[k] == position_token:
                    if k.startswith("<row_"):
                        k = k.replace(">", "")
                        rc = k.split("_")
                        row = int(rc[1])
                        col = int(rc[3])
                        max_row = max(max_row, row)
                        max_col = max(max_col, col)
                    else:
                        row = -1
                        col = -1
                    break
            block_index.append({"row": row, "col": col, "start": f + 2, "end": f + 66})

        # æ”¶é›†æ¯ä¸ªblockçš„attention map
        for token_index, layer_attention in enumerate(attentions):
            for layer, attention_map in enumerate(layer_attention):
                attention_map = attention_map.to(dtype=torch.float32).cpu().numpy()
                for bi in block_index:
                    start = bi["start"]
                    end = bi["end"]
                    att = attention_map[0, :, prompt_start_index:, start:end]
                    att = np.sum(att, axis=(0, 1))
                    if "att" not in bi:
                        bi["att"] = {}
                    if layer not in bi["att"]:
                        bi["att"][layer] = {}
                    bi["att"][layer].update({token_index: att})

        return block_index, max_row, max_col, attentions

    def create_attention_visualizations(self, block_index, max_row, max_col, attentions, image_path):
        """
        åˆ›å»ºæ³¨æ„åŠ›å¯è§†åŒ–å›¾åƒ
        
        Args:
            block_index: å›¾åƒå—ç´¢å¼•ä¿¡æ¯
            max_row: æœ€å¤§è¡Œæ•°
            max_col: æœ€å¤§åˆ—æ•°
            attentions: æ³¨æ„åŠ›ä¿¡æ¯
            image_path: åŸå§‹å›¾åƒè·¯å¾„
        """
        # ä»é…ç½®ä¸­è·å–alphaå€¼
        alpha = self.config["output"]["alpha"]
        
        # åˆå¹¶åŒä¸€å±‚çš„attention map
        for bi in block_index:
            att = bi["att"]
            for layer, att_map in att.items():
                layer_attention_map = np.zeros(64)
                for token_index, attention_map in att_map.items():
                    layer_attention_map += attention_map
                layer_attention_map = layer_attention_map.reshape(8, 8)
                # layer_attention_map = np.log(layer_attention_map + 1)
                layer_attention_map = layer_attention_map / np.max(layer_attention_map)
                layer_attention_map = layer_attention_map * 255
                layer_attention_map = layer_attention_map.astype(np.uint8)
                layer_attention_map = Image.fromarray(layer_attention_map)
                layer_attention_map = layer_attention_map.resize((512, 512), Image.NEAREST)
                layer_attention_map = np.array(layer_attention_map)
                if "view" not in bi:
                    bi["view"] = {}
                bi["view"][layer] = layer_attention_map

        # æ‹¼æ¥å›¾ç‰‡
        max_layer = len(attentions[0])
        block_size = 512
        block_layer_view = [np.zeros((2048, 2048)) for _ in range(max_layer)]
        whole_layer_view = []

        for bi in block_index:
            row = bi["row"] - 1
            col = bi["col"] - 1
            att = bi["view"]
            for layer, layer_attention_map in att.items():
                if row < 0 and col < 0:
                    whole_layer_view.append(layer_attention_map)
                    continue
                block_layer_view[layer][row * block_size:(row + 1) * block_size,
                col * block_size:(col + 1) * block_size] = layer_attention_map

        # æŠŠmapç›–åˆ°åŸå›¾ä¸Š,å¹¶ä¿å­˜å›¾ç‰‡
        image = Image.open(image_path)
        image = image.resize((max_col * block_size, max_row * block_size))
        image = np.array(image)

        # ä¿å­˜å—å±‚è§†å›¾
        self._save_block_layer_views(block_layer_view, image, max_row, max_col, block_size, alpha)

        # ä¿å­˜æ•´ä½“å±‚è§†å›¾
        self._save_whole_layer_views(whole_layer_view, image, alpha)

    def _save_block_layer_views(self, block_layer_view, image, max_row, max_col, block_size, alpha):
        """ä¿å­˜å—å±‚è§†å›¾"""
        for layer, layer_attention_map in enumerate(block_layer_view):
            layer_attention_map = layer_attention_map.astype(np.float32)
            save_image = np.zeros((layer_attention_map.shape[0], layer_attention_map.shape[1], 3))
            save_image[:image.shape[0], :image.shape[1], :] = image
            save_image[:, :, 0] = (1 - alpha) * save_image[:, :, 0] + alpha * layer_attention_map
            save_image[:, :, 1:] = (1 - alpha) * save_image[:, :, 1:]

            # æ·»åŠ è“è‰²è¾¹ç•Œçº¿ï¼ˆæ¯512åƒç´ ä¸€æ¡ï¼‰
            for i in range(1, max_row):
                save_image[i * block_size - 1:i * block_size + 1, :, 0] = 0
                save_image[i * block_size - 1:i * block_size + 1, :, 1] = 0
                save_image[i * block_size - 1:i * block_size + 1, :, 2] = 255  # è“è‰²
            for j in range(1, max_col):
                save_image[:, j * block_size - 1:j * block_size + 1, 0] = 0
                save_image[:, j * block_size - 1:j * block_size + 1, 1] = 0
                save_image[:, j * block_size - 1:j * block_size + 1, 2] = 255  # è“è‰²

            # æ·»åŠ ç»¿è‰²è¾¹ç•Œçº¿ï¼ˆæ¯64åƒç´ ä¸€æ¡ï¼‰
            pixel_per_token = 64
            for row in range(max_row):
                for col in range(max_col):
                    for i in range(1, 8):  # æ¯ä¸ªå¤§å—å†…æœ‰8ä¸ªå°å—
                        # æ°´å¹³ç»¿çº¿
                        y = row * block_size + i * pixel_per_token
                        if y < save_image.shape[0]:
                            save_image[y, col * block_size:(col + 1) * block_size, 0] = 0
                            save_image[y, col * block_size:(col + 1) * block_size, 1] = 255  # ç»¿è‰²
                            save_image[y, col * block_size:(col + 1) * block_size, 2] = 0

                        # å‚ç›´ç»¿çº¿
                        x = col * block_size + i * pixel_per_token
                        if x < save_image.shape[1]:
                            save_image[row * block_size:(row + 1) * block_size, x, 0] = 0
                            save_image[row * block_size:(row + 1) * block_size, x, 1] = 255  # ç»¿è‰²
                            save_image[row * block_size:(row + 1) * block_size, x, 2] = 0

            save_image = save_image.astype(np.uint8)
            save_image = Image.fromarray(save_image)
            save_image.save(os.path.join(self.save_path, f"block_layer_view_{layer}.png"))

    def _save_whole_layer_views(self, whole_layer_view, image, alpha):
        """ä¿å­˜æ•´ä½“å±‚è§†å›¾"""
        for layer, layer_attention_map in enumerate(whole_layer_view):
            layer_attention_map = layer_attention_map.astype(np.uint8)
            layer_attention_map = Image.fromarray(layer_attention_map)
            layer_attention_map = layer_attention_map.resize((image.shape[1], image.shape[0]), Image.NEAREST)
            layer_attention_map = np.array(layer_attention_map)
            save_image = image.copy()
            save_image[:, :, 0] = (1 - alpha) * save_image[:, :, 0] + alpha * layer_attention_map
            save_image[:, :, 1:] = (1 - alpha) * save_image[:, :, 1:]
            save_image = save_image.astype(np.uint8)
            save_image = Image.fromarray(save_image)
            save_image.save(os.path.join(self.save_path, f"whole_layer_view_{layer}.png"))

    def process_image(self, image_path, prompt_text=None):
        """
        å¤„ç†å›¾åƒå¹¶ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–
        
        Args:
            image_path: å›¾åƒè·¯å¾„
            prompt_text: æç¤ºæ–‡æœ¬ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤æç¤º
        """
        # è·å–å›¾ç‰‡åç§°ï¼ˆä¸å«åç¼€ï¼‰å¹¶åˆ›å»ºå¯¹åº”æ–‡ä»¶å¤¹
        image_name = os.path.splitext(os.path.basename(image_path))[0]
        image_save_path = os.path.join(self.save_path, str(self.lora_step), image_name)
        os.makedirs(image_save_path, exist_ok=True)

        # ä¸´æ—¶ä¿å­˜åŸå§‹save_path
        original_save_path = self.save_path
        # ä¿®æ”¹save_pathä¸ºæ–°åˆ›å»ºçš„æ–‡ä»¶å¤¹è·¯å¾„
        self.save_path = image_save_path

        # ç”Ÿæˆå¸¦æœ‰æ³¨æ„åŠ›çš„æ–‡æœ¬
        _, generated_ids = self.generate_with_attention(image_path, prompt_text)

        # å¤„ç†æ³¨æ„åŠ›å›¾
        block_index, max_row, max_col, attentions = self.process_attention_maps(generated_ids)

        # åˆ›å»ºæ³¨æ„åŠ›å¯è§†åŒ–
        self.create_attention_visualizations(block_index, max_row, max_col, attentions, image_path)

        # æ¢å¤åŸå§‹save_path
        self.save_path = original_save_path

    def process_config_images(self):
        # ä»é…ç½®æ–‡ä»¶ä¸­è·å–æµ‹è¯•å›¾åƒè·¯å¾„
        image_paths = self.config["test_images"]

        # å¤„ç†æ¯å¼ å›¾åƒ
        for image_path in image_paths:
            self.process_image(image_path)

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œé‡Šæ”¾æ˜¾å­˜"""
        if hasattr(self, 'model'):
            del self.model


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # åˆ›å»ºAttentionViewerå®ä¾‹ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶
    viewer = AttentionViewer("attention_view_config.yaml")

    viewer.process_config_images()
