from transformers import AutoProcessor, AutoModelForImageTextToText
from peft import PeftModel, LoraConfig  # å¯¼å…¥ LoraConfig
import torch
import os
import time
import json  # å¯¼å…¥ json
import pandas as pd
from transformers import TextStreamer
import yaml  # å¯¼å…¥yamlåº“
import subprocess
import requests
from openai import OpenAI
import base64
from PIL import Image
from io import BytesIO
import asyncio
from tqdm import tqdm


# åŠ è½½é…ç½®æ–‡ä»¶
def load_config(config_path="eval_config.yaml"):
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # å¤„ç†æ¨¡æ¿å­—ç¬¦ä¸²
    if "{lora_step}" in config["model"]["lora_adapter_path"]:
        config["model"]["lora_adapter_path"] = config["model"]["lora_adapter_path"].format(
            lora_step=config["model"]["lora_step"])

    if "{image_dir}" in config["eval"]["excel_path"] or "{lora_step}" in config["eval"]["excel_path"]:
        config["eval"]["excel_path"] = config["eval"]["excel_path"].format(
            image_dir=config["eval"]["image_dir"],
            lora_step=config["model"]["lora_step"])

    return config


# åŠ è½½é…ç½®
config = load_config()

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["HF_ENDPOINT"] = config["env"]["hf_endpoint"]
os.environ["ARK_API_KEY"] = config["env"]["ark_api_key"]

# åœæ­¢è¿è¡Œæ¨¡å‹
subprocess.run(['ollama', 'list'])
subprocess.run(['ollama', 'stop', config["translation"]["model"]])

# åŸºç¡€æ¨¡å‹çš„è·¯å¾„æˆ–Hugging Faceæ¨¡å‹ID
base_model_path = config["model"]["base_model_path_small"] if config["model"]["smol"] else config["model"][
    "base_model_path_large"]
lora_step = config["model"]["lora_step"]
skip_generate_and_translate = config["eval"]["skip_generate_and_translate"]
skip_score = config["eval"]["skip_score"]

# è®­ç»ƒå¥½çš„LoRAé€‚é…å™¨è·¯å¾„
lora_adapter_path = config["model"]["lora_adapter_path"]

# æµ‹è¯•å›¾åƒçš„ç›®å½•
image_dir = config["eval"]["image_dir"]

# Excel æ–‡ä»¶è·¯å¾„
excel_path = config["eval"]["excel_path"]

if not skip_generate_and_translate:
    # åŠ è½½å¤„ç†å™¨
    processor = AutoProcessor.from_pretrained(base_model_path)

    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model_for_lora = AutoModelForImageTextToText.from_pretrained(
        base_model_path,
        torch_dtype=torch.bfloat16,
        _attn_implementation="flash_attention_2"
    )

    if lora_step > 0:
        print("åŠ è½½LORA...")
        if config["model"]["base_adapter_path"] is not None:
            # åŠ è½½LoRAé€‚é…å™¨åˆ°åŸºç¡€æ¨¡å‹ï¼Œå¹¶ä¼ å…¥ä¿®æ­£åçš„config
            model = PeftModel.from_pretrained(base_model_for_lora, config["model"]["base_adapter_path"])
            model = model.merge_and_unload()
        else:
            model = base_model_for_lora

        model = PeftModel.from_pretrained(model, lora_adapter_path)
        model = model.merge_and_unload().eval()

        model = model.to("cuda").to(torch.bfloat16)
    else:
        print("ä½¿ç”¨åŸºç¡€æ¨¡å‹...")
        model = base_model_for_lora
        model = model.to("cuda").to(torch.bfloat16).eval()

    peak_mem = torch.cuda.max_memory_allocated()
    print(f"æ¨¡å‹å½“å‰å ç”¨æ˜¾å­˜: {peak_mem / 1024 ** 3:.2f} GB")
    total_params = sum(p.numel() for p in model.parameters())
    print(f"æ€»å‚æ•°: {total_params / 1e5:.2f} ä¸‡")
else:
    model = None
    processor = None

if os.path.exists(excel_path):
    df = pd.read_excel(excel_path)
    print(f"è¯»å–åˆ°excelæ–‡ä»¶ å…±{len(df)}è¡Œ")
    # æ·»åŠ ä¸€è¡Œåˆ†éš”
    df = pd.concat([df, pd.DataFrame(
        [{"image_file": "ä¸€ä¸€ä¸€",
          "output_str": "ä¸€ä¸€ä¸€",
          "output_str_beam": "ä¸€ä¸€ä¸€",
          "score": "ä¸€ä¸€ä¸€",
          "score_beam": "ä¸€ä¸€ä¸€",
          "reasoning": "ä¸€ä¸€ä¸€",
          "reasoning_beam": "ä¸€ä¸€ä¸€",
          "translate": "ä¸€ä¸€ä¸€",
          "translate_beam": "ä¸€ä¸€ä¸€"}])],
                   ignore_index=True)
else:
    df = pd.DataFrame(columns=["image_file",
                               "output_str",
                               "output_str_beam",
                               "score",
                               "score_beam",
                               "reasoning",
                               "reasoning_beam",
                               "translate",
                               "translate_beam"])


def caption_one_image(image_path):
    with torch.no_grad():
        prompts = ["Write a descriptive caption for this image in a formal tone.",
                   "Write a descriptive caption for this image in a casual tone.",
                   "Analyze this image like an art critic would with information about its composition, style, symbolism, the use of color, light, any artistic movement it might belong to, etc. ",
                   ]
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "url": image_path},
                    {"type": "text",
                     "text": prompts[0]},
                ]
            },
        ]
        st = time.time()

        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(model.device, dtype=torch.bfloat16)
        print(f"å¤„ç†è¾“å…¥è€—æ—¶: {time.time() - st:.4f} s")

        print(f'[Image]: {os.path.basename(image_path)}')
        print('ğŸ¤–ï¸: ', end='')
        # streamer = TextStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)

        # ä½¿ç”¨é…ç½®ä¸­çš„ç”Ÿæˆå‚æ•°
        gen_config = config["generation"]
        generated_ids = model.generate(
            **inputs,
            do_sample=gen_config["do_sample"],
            temperature=gen_config["temperature"],
            top_p=gen_config["top_p"],
            max_length=gen_config["max_length"],
            no_repeat_ngram_size=gen_config["no_repeat_ngram_size"],
            repetition_penalty=gen_config["repetition_penalty"],
        )

        beam_config = gen_config["beam_generation"]
        generated_ids2 = model.generate(
            **inputs,
            num_beams=beam_config["num_beams"],
            max_length=beam_config["max_length"],
            early_stopping=beam_config["early_stopping"],
            no_repeat_ngram_size=beam_config["no_repeat_ngram_size"],
            length_penalty=beam_config["length_penalty"],
            repetition_penalty=beam_config["repetition_penalty"],
        )
        # å¦‚æœè¿˜éœ€è¦å®Œæ•´æ–‡æœ¬ï¼Œå¯ä»¥å†decodeä¸€æ¬¡
        generated_texts = processor.batch_decode(
            generated_ids,
            skip_special_tokens=True,
        )
        generated_texts.extend(processor.batch_decode(
            generated_ids2,
            skip_special_tokens=True,
        ))
        output_str = generated_texts[0].split("Assistant:")[-1].strip()
        output_str2 = generated_texts[1].split("Assistant:")[-1].strip()
        print(output_str)
        return st, output_str, output_str2, generated_ids, generated_ids2


def caption_dir(image_dir):
    if not os.path.exists(image_dir):
        return
    global df
    global_token_speed = [0, 0]
    global_chart_speed = [0, 0]
    # éå†å›¾åƒæ–‡ä»¶è¿›è¡Œæµ‹è¯•
    for image_file in os.listdir(image_dir):
        if image_file.endswith(".xlsx"):  # è·³è¿‡éå›¾åƒæ–‡ä»¶
            continue

        image_path = os.path.join(image_dir, image_file)
        if os.path.isdir(image_path):
            continue

        st, output_str, output_str2, generated_ids, generated_ids2 = caption_one_image(image_path)

        et = time.time() - st
        token_len = len(generated_ids[0]) + len(generated_ids2[0])
        chart_len = len(output_str) + len(output_str2)
        token_speed = token_len / et
        chart_speed = chart_len / et
        global_token_speed[0] += token_len
        global_token_speed[1] += et
        global_chart_speed[0] += chart_len
        global_chart_speed[1] += et
        print(f"token_speed:{token_speed:.4f} tokens/s")
        print(f"chart_speed:{chart_speed:.4f} chart/s")
        print(f"æ€»è€—æ—¶:{et:.4f} s")
        print("-" * 50)

        # æ–°å¢ï¼šå°†è¾“å‡ºå†™å…¥Excelæ–‡ä»¶
        new_row = {
            "image_file": image_path,
            "output_str": output_str,
            "output_str_beam": output_str2,
            "score": None,
            "score_beam": None,
            "reasoning": None,
            "reasoning_beam": None,
            "translate": None,
            "translate_beam": None
        }
        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

    print(f"global_token_speed:{global_token_speed[0] / global_token_speed[1]:.4f} tokens/s")
    print(f"global_chart_speed:{global_chart_speed[0] / global_chart_speed[1]:.4f} chart/s")


if skip_generate_and_translate:
    safe_len = len([i for i in os.listdir(image_dir) if not i.endswith(".xlsx")])
else:
    caption_dir(image_dir)
    safe_len = df.shape[0]
    caption_dir(os.path.join(image_dir, "R18"))

# å¸è½½æ¨¡å‹ï¼Œé‡Šæ”¾æ˜¾å­˜
del model
torch.cuda.empty_cache()

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    base_url=config["scoring"]["base_url"],
    api_key=os.environ.get("ARK_API_KEY"),
)


def translate_with_ollama(text):
    # ä½¿ç”¨chatæ¨¡å¼ï¼Œæ„é€ ç¿»è¯‘æç¤ºè¯
    url = "http://localhost:11434/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    messages = [
        {"role": "system",
         "content": config["translation"]["system_prompt"]},
        {"role": "user", "content": text}
    ]
    payload = {
        "model": config["translation"]["model"],
        "messages": messages,
        "max_tokens": config["translation"]["max_tokens"],
        "temperature": config["translation"]["temperature"]
    }
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=240)
        if response.status_code == 200:
            data = response.json()
            # å…¼å®¹ä¸åŒè¿”å›æ ¼å¼
            if "choices" in data and len(data["choices"]) > 0:
                res = data["choices"][0]["message"]["content"].strip()
                if "</think>" in res:
                    return res.split("</think>")[1].strip()
                else:
                    return res
            else:
                return ""
        else:
            print(f"ç¿»è¯‘è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
            return ""
    except Exception as e:
        print(f"ç¿»è¯‘å¤±è´¥: {e}")
        return ""


def image_to_base64(input_path, output_quality=None):
    try:
        # è¯»å–å›¾ç‰‡
        with Image.open(input_path) as img:
            # è½¬æ¢ä¸ºRGBæ¨¡å¼ï¼ˆJPEGä¸æ”¯æŒRGBAï¼‰
            if img.mode == 'RGBA':
                img = img.convert('RGB')
            # åˆ›å»ºå†…å­˜ç¼“å†²åŒº
            buffer = BytesIO()

            if img.size[0] > 1920 or img.size[1] > 1920:
                # æŒ‰ç…§æ¯”ä¾‹ç¼©æ”¾å›¾ç‰‡
                scale = max(img.size[0] / 1920, img.size[1] / 1920)
                new_size = (int(img.size[0] / scale), int(img.size[1] / scale))
                img = img.resize(new_size, Image.LANCZOS)

            img.save(buffer, format='PNG', quality=90)

            # è·å–äºŒè¿›åˆ¶æ•°æ®
            img_bytes = buffer.getvalue()

            # è½¬æ¢ä¸ºBase64ç¼–ç 
            encoded = base64.b64encode(img_bytes).decode('utf-8')

            return encoded
    except FileNotFoundError:
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ–‡ä»¶ '{input_path}'")
        return None
    except Exception as e:
        print(f"é”™è¯¯ï¼šå¤„ç†å›¾ç‰‡æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š{e}")
        return None


def score_with_doubao(text, image_path):
    # è¯·ç¡®ä¿æ‚¨å·²å°† API Key å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ ARK_API_KEY ä¸­
    # åˆå§‹åŒ–Arkå®¢æˆ·ç«¯ï¼Œä»ç¯å¢ƒå˜é‡ä¸­è¯»å–æ‚¨çš„API Key
    prompt = config["scoring"]["prompt"].format(text=text)

    response = client.chat.completions.create(
        model=config["scoring"]["model"],
        messages=[
            {
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/png;base64,{image_to_base64(image_path)}"
                        },
                    },
                    {"type": "text", "text": prompt},
                ],
            }
        ],
    )

    reasoning = response.choices[0].message.reasoning_content
    answer = response.choices[0].message.content
    print(reasoning + "\n" + answer)
    if "<score>" in answer:
        answer = answer.split("<score>")[-1].split("</score>")[0].strip()
    return reasoning, answer


async def async_translate_with_ollama(output_str):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, translate_with_ollama, output_str)


async def async_score_with_doubao(text, image_path):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, score_with_doubao, text, image_path)


async def async_main(skip_translate=False):
    global df
    if not skip_translate:
        # åˆ›å»ºä¸€ä¸ªä¸DataFrameé•¿åº¦ç›¸åŒçš„ç©ºåˆ—è¡¨
        translations = [""] * len(df)
        translations_beam = [""] * len(df)

        # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼Œå¹¶ä¿æŒä¸DataFrameç´¢å¼•çš„å¯¹åº”å…³ç³»
        tasks = []
        for idx, output_str in enumerate(df["output_str"]):
            if output_str == "ä¸€ä¸€ä¸€":
                continue
            # è·³è¿‡å·²ç»ç¿»è¯‘è¿‡çš„è¡Œ
            if not pd.isna(df.at[idx, "translate"]):
                translations[idx] = df.at[idx, "translate"]
                continue
            # å°†ç´¢å¼•ä¸ä»»åŠ¡ä¸€èµ·å­˜å‚¨ï¼Œä»¥ä¾¿åç»­åŒ¹é…
            task = asyncio.create_task(async_translate_with_ollama(output_str))
            tasks.append((idx, "output_str", task))

        for idx, output_str_beam in enumerate(df["output_str_beam"]):
            if output_str_beam == "ä¸€ä¸€ä¸€":
                continue
            # è·³è¿‡å·²ç»ç¿»è¯‘è¿‡çš„è¡Œ
            if not pd.isna(df.at[idx, "translate_beam"]):
                translations_beam[idx] = df.at[idx, "translate_beam"]
                continue
            # å°†ç´¢å¼•ä¸ä»»åŠ¡ä¸€èµ·å­˜å‚¨ï¼Œä»¥ä¾¿åç»­åŒ¹é…
            task = asyncio.create_task(async_translate_with_ollama(output_str_beam))
            tasks.append((idx, "output_str_beam", task))

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        for idx, col, task in tqdm([(i, c, t) for i, c, t in tasks], total=len(tasks), desc="Translating"):
            result = await task
            # å°†ç¿»è¯‘ç»“æœæ”¾å…¥å¯¹åº”çš„ä½ç½®
            if col == "output_str_beam":
                translations_beam[idx] = result
            else:
                translations[idx] = result

        # å°†ç¿»è¯‘ç»“æœæ·»åŠ åˆ°DataFrame
        df["translate"] = translations
        df["translate_beam"] = translations_beam

        # ä¿å­˜åˆ° Excel
        df.to_excel(excel_path, index=False)
        print("å®Œæˆç¿»è¯‘, å·²ä¿å­˜")
        print("å¼€å§‹è¯„åˆ†...")

    # =============================è¿›è¡Œè¯„åˆ†================================
    if skip_score:
        return
    scores = [""] * len(df)
    scores_beam = [""] * len(df)
    reasoning_col = [""] * len(df)
    reasoning_col_beam = [""] * len(df)

    # æ·»åŠ ä¿¡å·é‡æ§åˆ¶å¹¶å‘
    sem = asyncio.Semaphore(config["scoring"]["concurrent_tasks"])  # å¯æ ¹æ®éœ€è¦è°ƒæ•´å¹¶å‘æ•°

    # å®šä¹‰å¸¦ä¿¡å·é‡é™åˆ¶çš„ä»»åŠ¡åŒ…è£…å‡½æ•°
    async def limited_task(coro):
        async with sem:  # åŒä¸€æ—¶é—´æœ€å¤šè¿è¡Œsem.valueä¸ªä»»åŠ¡
            return await coro

    # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼Œå¹¶ä¿æŒä¸DataFrameç´¢å¼•çš„å¯¹åº”å…³ç³»
    tasks = []
    for idx in range(safe_len):
        if df.at[idx, "output_str"] == "ä¸€ä¸€ä¸€":
            continue
        # è·³è¿‡å·²ç»ç¿»è¯‘è¿‡çš„è¡Œ
        if not pd.isna(df.at[idx, "score"]):
            scores[idx] = df.at[idx, "score"]
            continue
        # å°†ç´¢å¼•ä¸ä»»åŠ¡ä¸€èµ·å­˜å‚¨ï¼Œä»¥ä¾¿åç»­åŒ¹é…
        image_path = df.at[idx, "image_file"]
        output_str = df.at[idx, "output_str"]
        # åŒ…è£…ä»»åŠ¡ä»¥åº”ç”¨ä¿¡å·é‡é™åˆ¶
        task = asyncio.create_task(limited_task(async_score_with_doubao(output_str, image_path)))
        tasks.append((idx, "output_str", task))

    for idx in range(safe_len):
        if df.at[idx, "output_str_beam"] == "ä¸€ä¸€ä¸€":
            continue
        # è·³è¿‡å·²ç»ç¿»è¯‘è¿‡çš„è¡Œ
        if not pd.isna(df.at[idx, "score_beam"]):
            scores_beam[idx] = df.at[idx, "score_beam"]
            continue
        # å°†ç´¢å¼•ä¸ä»»åŠ¡ä¸€èµ·å­˜å‚¨ï¼Œä»¥ä¾¿åç»­åŒ¹é…
        image_path = df.at[idx, "image_file"]
        output_str_beam = df.at[idx, "output_str_beam"]
        # åŒ…è£…ä»»åŠ¡ä»¥åº”ç”¨ä¿¡å·é‡é™åˆ¶
        task = asyncio.create_task(limited_task(async_score_with_doubao(output_str_beam, image_path)))
        tasks.append((idx, "output_str_beam", task))

    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for idx, col, task in tqdm([(i, c, t) for i, c, t in tasks], total=len(tasks), desc="Scoring"):
        reasoning, answer = await task
        # å°†ç¿»è¯‘ç»“æœæ”¾å…¥å¯¹åº”çš„ä½ç½®
        if col == "output_str_beam":
            scores_beam[idx] = answer
            reasoning_col_beam[idx] = reasoning
        else:
            scores[idx] = answer
            reasoning_col[idx] = reasoning

    # å°†è¯„åˆ†ç»“æœæ·»åŠ åˆ°DataFrame
    df["score"] = scores
    df["score_beam"] = scores_beam
    df["reasoning"] = reasoning_col
    df["reasoning_beam"] = reasoning_col_beam


asyncio.run(async_main(skip_generate_and_translate))

# ä¿å­˜åˆ° Excel
df.to_excel(excel_path, index=False)

print("Evaluation script finished.")
subprocess.run(['ollama', 'stop', config["translation"]["model"]])
